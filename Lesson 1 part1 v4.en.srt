00:00:00.680,00:00:06.499
So hello everybody and welcome to Deep Learning
for Coders. Lesson One.

00:00:06.499,00:00:16.079
This is the fourth year that we've done this.
but it's a very different and very special

00:00:16.079,00:00:17.919
version for a number of reasons.

00:00:17.920,00:00:23.260
The first reason it's different is because
we are bringing it to you live from day number

00:00:23.260,00:00:24.910
one of a complete shutdown.

00:00:24.910,00:00:29.490
Oh. not a complete shutdown. but nearly a
complete shutdown of San Francisco.

00:00:29.490,00:00:34.430
We're going to be recording it over the next
two months in the midst of this global pandemic.

00:00:34.430,00:00:39.150
So if things seem a little crazy sometimes
in this course. I apologize.

00:00:39.150,00:00:43.300
So that's why this is happening.

00:00:43.300,00:00:53.190
The other reason it's special is because it's.
we're trying to make this our definitive version.

00:00:53.190,00:00:54.419
right.

00:00:54.420,00:00:58.950
Since we've been doing this for a while now.
we've finally got to the point where we almost

00:00:58.950,00:01:01.400
feel like we know what we're talking about.

00:01:01.400,00:01:07.670
To the point that Sylvain and I have actually
written a book and we've actually written

00:01:07.670,00:01:12.210
a piece of software from scratch. called the
fastai library version 2.

00:01:12.210,00:01:18.210
We've written a peer-reviewed paper about
this library.

00:01:18.210,00:01:24.699
So this is kind of designed to be like the
version of the course that is hopefully going

00:01:24.700,00:01:27.560
to last a while.

00:01:27.560,00:01:32.310
The syllabus is based very closely on this
book. right.

00:01:32.310,00:01:38.700
So if you want to read along properly as you
go. please buy it.

00:01:38.700,00:01:43.700
And I say “please buy it” because actually
the whole thing is also available for free

00:01:43.700,00:01:45.770
in the form of Jupyter notebooks.

00:01:45.770,00:01:54.170
And that is thanks to the huge generosity
of O'Reilly Media. who have let us do that.

00:01:54.170,00:02:03.759
So you'll be able to see on the website for
the course how to kind of access all this.

00:02:03.759,00:02:12.530
but here is the fastbook repo where you can
read the whole damn thing.

00:02:12.530,00:02:18.549
At the moment as you see. it's a draft. but
by the time you read this. it won't be.

00:02:18.549,00:02:26.659
So we have a big request here which is - the
deal is this - you can read this thing for

00:02:26.659,00:02:34.649
free as Jupyter notebooks. but that is not
as convenient as reading it on a Kindle or

00:02:34.650,00:02:36.340
in a paper book or whatever.

00:02:36.340,00:02:39.769
So. please don't turn this into a PDF. right.

00:02:39.769,00:02:46.449
Please don't turn it into a form designed
more for reading. because kind of the whole

00:02:46.449,00:02:50.159
point is that you'll buy it.

00:02:50.159,00:02:58.810
Don't take advantage of O'Reilly's generosity
by creating the thing that you know they're

00:02:58.810,00:02:59.859
not giving you for free.

00:02:59.859,00:03:05.450
And that's actually explicitly the license
under which we're providing this as well.

00:03:05.450,00:03:10.530
So it's mainly a request. being a decent human
being.

00:03:10.530,00:03:14.680
If you see somebody else not being a decent
human being and stealing the book version

00:03:14.680,00:03:19.189
of the book. please tell them. “Please don't
do that. it's not nice.”

00:03:19.189,00:03:21.499
And don't be that person.

00:03:21.499,00:03:28.069
So either way. you can read along with the
syllabus in the book.

00:03:28.069,00:03:35.268
There's a couple of different versions of
these notebooks. right.

00:03:35.269,00:03:47.540
There is the. there's the full notebook that
has the entire prose. pictures. everything.

00:03:47.540,00:03:55.858
Now we actually wrote a system to turn notebooks
into a printed book and sometimes that looks

00:03:55.859,00:03:56.969
kind of weird.

00:03:56.969,00:04:05.840
For example. here's a weird looking table
and if you look in the actual book. it actually

00:04:05.840,00:04:07.919
looks like a proper table. right.

00:04:07.919,00:04:13.409
So sometimes you'll see like little weird
bits. okay. they are not mistakes they are

00:04:13.409,00:04:19.159
bits where we can add information to help
our book turn into a proper nice book so just

00:04:19.159,00:04:20.159
just ignore them.

00:04:20.159,00:04:23.389
Now when I say we. who is we?

00:04:23.389,00:04:30.070
While I mentioned one important part of the
we is Sylvain.

00:04:30.070,00:04:37.789
Sylvain is my co-author of the book and fastai
version 2 library. so he is my partner in

00:04:37.789,00:04:39.310
crime here.

00:04:39.310,00:04:48.319
The other key “we” here is Rachel Thomas
and so maybe Rachel you can come and say hello.

00:04:48.319,00:04:50.230
She is the co-founder of fastai.

00:04:50.230,00:04:58.680
Hello yes I am the co-founder of fastai and
I am also. lower sorry taller than Jeremy.

00:04:58.680,00:05:04.300
and I am the founding director of the Center
for Applied Data Ethics at the University

00:05:04.300,00:05:06.030
of San Francisco.

00:05:06.030,00:05:09.929
Really excited to be a part of this course
and I will be the voice you hear asking questions

00:05:09.930,00:05:13.020
from the forums.

00:05:13.020,00:05:18.948
Rachel and Sylvain are also the people in
this group who actually understand math.

00:05:18.949,00:05:21.219
I am a mere philosophy graduate.

00:05:21.219,00:05:23.430
Rachel has a PhD.

00:05:23.430,00:05:29.639
Sylvain has written 10 books about math so
if the math questions come along it's possible

00:05:29.639,00:05:32.800
I may pass them along.

00:05:32.800,00:05:36.610
But it is very nice to have an opportunity
to work with people who understand this topic

00:05:36.610,00:05:37.699
so well.

00:05:37.699,00:05:45.960
Yes Yes Rachael. did you wanna sure oh thank
you.

00:05:45.960,00:05:52.739
As Rachel mentioned the other area where she
is you know real. has real world-class expertise

00:05:52.740,00:06:00.629
is data ethics. she is the founding director
of the Centre for Applied Data Ethics. at

00:06:00.629,00:06:02.389
the University of San Francisco.

00:06:02.389,00:06:03.389
Thank you.

00:06:03.389,00:06:08.139
We are gonna be talking about data ethics
throughout the course because well we happen

00:06:08.139,00:06:12.900
to think it's very important and so for those
parts. although I'll generally be presenting

00:06:12.900,00:06:19.549
them they will be on the whole based on Rachel's
Rachel's work because she actually knows what

00:06:19.550,00:06:21.919
she's talking about.

00:06:21.919,00:06:26.479
Although thanks to her I kind of know a bit
about what I am talking about too.

00:06:26.479,00:06:31.289
Right. so that's that.

00:06:31.289,00:06:38.438
So should you be here. is there any point
in you attempting to understand (I thought

00:06:38.439,00:06:43.460
I pressed the right button). understand deep
learning.

00:06:43.460,00:06:49.120
Ok so what do you know should you should you
be here.

00:06:49.120,00:06:55.330
Is there any point you attempting to learn
deep learning or are you too stupid or you

00:06:55.330,00:07:01.000
don't have enough fast resources or whatever.
because that's what a lot of people are telling

00:07:01.000,00:07:02.000
us.

00:07:02.000,00:07:07.370
They are saying you need teams of PhDs and
massive data centers full of GPUs. otherwise

00:07:07.370,00:07:08.550
it's pointless.

00:07:08.550,00:07:13.449
Don't worry that is not at all true. couldn't
be further from the truth.

00:07:13.449,00:07:21.740
In fact that the vast majority. sorry. a lot
of world-class research and world-class industry

00:07:21.740,00:07:33.689
projects have come out of fastai alumni and
fastai library-based projects and and elsewhere

00:07:33.689,00:07:42.789
which are created on a single GPU using a
few dozen or a few hundred data points from

00:07:42.789,00:07:51.289
people that have no graduate level technical
expertise. or in my case. I have no undergraduate

00:07:51.289,00:07:52.550
level technical expertise.

00:07:52.550,00:07:55.080
I'm just a philosophy major.

00:07:55.080,00:07:59.719
So there is - and we'll see it throughout
the course -- but there is lots and lots and

00:07:59.719,00:08:03.770
lots of clear empirical evidence that you
don't need lots of math. you don't need lots

00:08:03.770,00:08:08.250
of data. you don't need lots of expensive
computers to do great stuff. with deep learning.

00:08:08.250,00:08:10.240
So just bear with us.

00:08:10.240,00:08:12.229
You'll be fine.

00:08:12.229,00:08:15.109
To do this course. you do need to code.

00:08:15.110,00:08:18.750
Preferably. you know how to code in Python.

00:08:18.750,00:08:21.900
But if you've done other languages. you can
learn Python.

00:08:21.900,00:08:26.938
If the only languages you've done is something
like Matlab. where you've used it more like

00:08:26.939,00:08:32.570
a scripty kind of thing. you might find it
a bit - You will find it a bit heavier going.

00:08:32.570,00:08:35.880
But that's okay. stick with it.

00:08:35.880,00:08:40.990
You can learn Python as you go.

00:08:40.990,00:08:42.690
Is there any point learning deep learning?

00:08:42.690,00:08:46.220
Is it any good at stuff?

00:08:46.220,00:08:53.920
If you are hoping to build a brain. that is
an AGI. I cannot promise we're gonna help

00:08:53.920,00:08:55.099
you with that.

00:08:55.100,00:08:58.870
And AGI stands for: Artificial General Intelligence.

00:08:58.870,00:09:00.339
Thank you.

00:09:00.339,00:09:06.870
What I can tell you though. is that in all
of these areas. deep learning is the best-known

00:09:06.870,00:09:12.760
approach. to at least many versions of all
of these things.

00:09:12.760,00:09:18.130
So it is not speculative at this point whether
this is a useful tool.

00:09:18.130,00:09:21.880
It's a useful tool in lots and lots and lots
of places.

00:09:21.880,00:09:22.880
Extremely useful tool.

00:09:22.880,00:09:29.150
And in many of these cases. it is equivalent
to or better than human performance.

00:09:29.150,00:09:34.180
At least according to some particular narrow
definition of things that humans do in these

00:09:34.180,00:09:36.250
kinds of areas.

00:09:36.250,00:09:38.899
So deep learning is pretty amazing.

00:09:38.899,00:09:43.660
And if you want to pause the video here and
have a look through and try and pick some

00:09:43.660,00:09:48.600
things out that you think might look interesting
and type that keyword and “deep learning”

00:09:48.600,00:09:49.600
into Google.

00:09:49.600,00:09:55.360
And you'll find lots of papers and examples
and stuff like that.

00:09:55.360,00:09:59.800
Deep learning comes from a background of neural
networks.

00:09:59.800,00:10:05.240
As you'll see deep learning is just a type
of neural network learning.

00:10:05.240,00:10:06.920
A deep one.

00:10:06.920,00:10:08.650
We'll describe exactly what that means later.

00:10:08.650,00:10:11.660
And neural networks are certainly not a new
thing.

00:10:11.660,00:10:17.490
They go back at least to 1943. when McCulloch
and Pitts created a mathematical model of

00:10:17.490,00:10:19.310
an artificial neuron.

00:10:19.310,00:10:23.638
And got very excited about where that could
get to.

00:10:23.639,00:10:31.790
And then in the 50's. Frank Rosenblatt then
built on top of that - he basically created

00:10:31.790,00:10:36.319
some subtle changes to that mathematical model.

00:10:36.320,00:10:40.020
And he thought that with these subtle changes
“we could witness the birth of the machine

00:10:40.020,00:10:44.399
that is capable of perceiving. recognizing
and identifying its surroundings without any

00:10:44.399,00:10:46.740
human training or control”.

00:10:46.740,00:10:51.200
And he oversaw the building of this - extraordinary
thing.

00:10:51.200,00:10:54.860
The Mark 1 Perceptron at Cornell.

00:10:54.860,00:11:00.269
So that was I think. this picture was 1961.

00:11:00.269,00:11:04.610
Thankfully nowadays. we don't have to build
neural networks by running the damn wires

00:11:04.610,00:11:07.089
from neuron to neuron (artificial neuron to
artificial neuron).

00:11:07.089,00:11:10.800
But you can kind of see the idea; lot of connections
going on.

00:11:10.800,00:11:14.219
And you'll hear the word connection a lot.
in this course. because that's what it's all

00:11:14.220,00:11:16.880
about.

00:11:16.880,00:11:19.790
Then we had the first AI winter. as it was
known.

00:11:19.790,00:11:26.899
Which really. to a strong degree happened
because an MIT professor named Marvin Minsky

00:11:26.899,00:11:31.810
and Papert wrote a book called perceptrons
about Rosenblatt's invention in which they

00:11:31.810,00:11:39.619
pointed out that a single layer of these artificial
neuron devices. actually couldn't learn some

00:11:39.620,00:11:40.620
critical things.

00:11:40.620,00:11:45.990
It was like impossible for them to learn something
as simple as the Boolean XOR operator.

00:11:45.990,00:11:51.570
In the same book. they showed that using multiple
layers of the devices actually would fix the

00:11:51.570,00:11:52.570
problem.

00:11:52.570,00:11:54.880
People ignore - didn't notice that part of
the book.

00:11:54.880,00:12:00.639
And only noticed the limitation and people
basically decided that neural networks are

00:12:00.639,00:12:01.829
gonna go nowhere.

00:12:01.829,00:12:07.000
And they kind of largely disappeared. for
decades.

00:12:07.000,00:12:09.829
Until. in some ways. 1986.

00:12:09.829,00:12:16.099
A lot happened in the meantime but there was
a big thing in 1986. which is: MIT released

00:12:16.100,00:12:22.560
a thing called a book. a series of two volumes
of book called. “Parallel Distributed Processing”.

00:12:22.560,00:12:28.020
In which they described this thing they call
parallel distributed processing. where you

00:12:28.020,00:12:35.769
have a bunch of processing units. that have
some state of activation and some output function

00:12:35.769,00:12:41.709
and some pattern of connectivity and some
propagation rule and some activation rule

00:12:41.709,00:12:44.910
and some learning rule. operating in an environment.

00:12:44.910,00:12:50.540
And then they described how things that met
these requirements. could in theory. do all

00:12:50.541,00:12:52.310
kinds of amazing work.

00:12:52.310,00:12:56.459
And this was the result of many. many researchers
working together.

00:12:56.459,00:13:02.040
There was a whole group involved in this project.
which resulted in this very. very important

00:13:02.040,00:13:03.139
book.

00:13:03.139,00:13:09.069
And so. the interesting thing to me. is that
if you - as you go through this course come

00:13:09.069,00:13:15.430
back and have a look at this picture and you'll
see we are doing exactly these things.

00:13:15.430,00:13:21.380
Everything we're learning about really is
how do you do each of these eight things?

00:13:21.380,00:13:25.420
That is interesting that they include the
environment because that's something which

00:13:25.420,00:13:28.329
very often. data scientists ignore.

00:13:28.329,00:13:31.800
Which is - you build a model. you've trained
it. it's learned something.

00:13:31.800,00:13:33.459
What's the context it works in?

00:13:33.459,00:13:40.040
And we're talking about that. quite a bit
over the next couple of lessons as well.

00:13:40.040,00:13:47.709
So in the 80's. during and after this was
released people started building in this second

00:13:47.709,00:13:51.660
layer of neurons. avoiding Minsky's problem.

00:13:51.660,00:13:59.860
And in fact. it was shown. that it was mathematically
provable. that by adding that one extra layer

00:13:59.860,00:14:07.790
of neurons. it was enough to allow any mathematical
model to be approximated to any level of accuracy.

00:14:07.790,00:14:10.160
with these neural networks.

00:14:10.160,00:14:13.689
And so that was like the exact opposite of
the Minsky thing.

00:14:13.690,00:14:17.440
That was like: “Hey there's nothing we can't
do.

00:14:17.440,00:14:19.649
Provably there's nothing we can't do.”

00:14:19.649,00:14:23.540
And so that was kind of when I started getting
involved in neural networks.

00:14:23.540,00:14:25.189
So I was - a little bit later.

00:14:25.190,00:14:28.560
I guess I was getting involved in the early
90s.

00:14:28.560,00:14:30.779
And they were very widely used in industry.

00:14:30.779,00:14:34.970
I was using them for very boring things like
targeted marketing for retail banks.

00:14:34.970,00:14:40.319
They tended to be big companies with lots
of money that were using them.

00:14:40.319,00:14:46.389
And it certainly though was true that often
the networks were too big or slow to be useful.

00:14:46.389,00:14:51.870
They were certainly useful for some things.
but they - you know they never felt to me

00:14:51.870,00:14:55.820
like they were living up to the promise for
some reason.

00:14:55.820,00:15:02.070
Now what I didn't know. and nobody I personally
met knew was that actually there were researchers

00:15:02.070,00:15:08.510
that had shown 30 years ago that to get practical
good performance. you need more layers of

00:15:08.510,00:15:09.510
neurons.

00:15:09.510,00:15:14.430
Even though mathematically. theoretically.
you can get as accurate as you want with just

00:15:14.430,00:15:16.019
one extra layer.

00:15:16.019,00:15:20.850
To do it with good performance. you need more
layers.

00:15:20.850,00:15:26.100
So when you add more layers to a neural network
you get deep learning.

00:15:26.100,00:15:29.470
So deep doesn't mean anything like mystical.

00:15:29.470,00:15:32.139
It just means more layers.

00:15:32.139,00:15:35.800
More layers than just adding the one extra
one.

00:15:35.800,00:15:39.459
So thanks to that. neural nets are now living
up to their potential.

00:15:39.460,00:15:42.680
As we saw in that like what's deep learning
good at thing.

00:15:42.680,00:15:46.359
So we could now say that Rosenblatt was right.

00:15:46.360,00:15:52.600
We have a machine that's capable of perceiving.
recognising and identifying its surroundings

00:15:52.600,00:15:54.720
without any human training or control.

00:15:54.720,00:15:56.360
That is - That's definitely true.

00:15:56.360,00:16:00.420
I don't think there's anything controversial
about that statement based on the current

00:16:00.420,00:16:01.469
technology.

00:16:01.470,00:16:04.939
So we're gonna be learning how to do that.

00:16:04.939,00:16:09.290
We're gonna be learning how to do that in
exactly the opposite way. of probably all

00:16:09.290,00:16:13.770
of the other math and technical education
you've had.

00:16:13.770,00:16:24.949
We are not gonna start with a two-hour lesson
about the sigmoid function or a study of linear

00:16:24.949,00:16:29.750
algebra or a refresher course on calculus.

00:16:29.750,00:16:37.709
And the reason for that. is that people who
study how to teach and learn have found that

00:16:37.709,00:16:41.638
is not the right way to do it for most people.

00:16:41.639,00:16:50.019
For most people - So we work a lot based on
the work of Professor David Perkins from Harvard

00:16:50.019,00:16:56.740
and others who work at similar things. who
talk about this idea of playing the whole

00:16:56.740,00:16:57.740
game.

00:16:57.740,00:17:01.260
And so playing the whole game is like it's
based on the sports analogy if you're gonna

00:17:01.260,00:17:03.949
teach somebody baseball.

00:17:03.949,00:17:10.790
You don't take them out into a classroom and
start teaching them about the physics of a

00:17:10.790,00:17:20.319
parabola. and how to stitch a ball. and a
three-part history of 100 years of baseball

00:17:20.319,00:17:24.290
politics. and then 10 years later. you let
them watch a game.

00:17:24.290,00:17:27.050
And then 20 years later. you let them play
a game.

00:17:27.050,00:17:31.930
Which is kind of like how math education is
being done. right?

00:17:31.930,00:17:37.180
Instead with baseball. step one is to say.
hey. let's go and watch some baseball.

00:17:37.180,00:17:38.180
What do you think?

00:17:38.180,00:17:39.180
That was fun. right?

00:17:39.180,00:17:42.060
See that guy there...he took a run there…before
the other guy throws a ball over there...hey.

00:17:42.060,00:17:44.080
you want to try having a hit?

00:17:44.080,00:17:47.379
Okay. so you're going to hit the ball. and
then I have to try to catch it. then he has

00:17:47.380,00:17:52.940
to run.run.run over there...and so from step
one. you are playing the whole game.

00:17:52.940,00:17:58.920
And just to add to that. when people start.
they often may not have a full team. or be

00:17:58.920,00:18:03.620
playing the full nine innings. but they still
have a sense of what the game is. a kind of

00:18:03.620,00:18:05.389
a big picture idea.

00:18:05.390,00:18:12.510
So. there is lots and lots of reasons that
this helps most human beings. (though) not

00:18:12.510,00:18:14.050
everybody. right?

00:18:14.050,00:18:19.430
There's a small percentage of people who like
to build things up from the foundations and

00:18:19.430,00:18:24.270
the principles. and not surprisingly. they
are massively overrepresented in a university

00:18:24.270,00:18:28.510
setting. because the people who get to be
academics are the people who thrive with.

00:18:28.510,00:18:33.350
(according) to me. the upside down way of
how things are taught.

00:18:33.350,00:18:40.449
But outside of universities most people learn
best in this top-down way. where you start

00:18:40.450,00:18:42.390
with the full context.

00:18:42.390,00:18:47.290
So step number two in the seven principles.
and I'm only going to mention the first three.

00:18:47.290,00:18:49.620
is to make the game worth playing.

00:18:49.620,00:18:53.590
Which is like. if you're playing baseball.
you have a competition.

00:18:53.590,00:19:00.139
You know. you score. you try and win. you
bring together teams from around the community

00:19:00.140,00:19:02.350
and you have people try to beat each other.

00:19:02.350,00:19:08.639
And you have leaderboards. like who's got
the highest number of runs or whatever.

00:19:08.640,00:19:14.800
So this is all about making sure that the
thing you're doing. you're doing it properly.

00:19:14.800,00:19:23.000
You're making it the whole thing. you're providing
the context and the interest.

00:19:23.000,00:19:30.490
So. for the fastai approach to learning deep
learning. what this means is that today we're

00:19:30.490,00:19:33.200
going to train models end to end.

00:19:33.200,00:19:38.350
We're going to actually train models. and
they won't just be crappy models.

00:19:38.350,00:19:45.070
They will be state-of-the-art world-class
models from today. and we can try to have

00:19:45.070,00:19:50.909
you build your own state-of-the-art world-class
models from either today or next lesson. depending

00:19:50.910,00:19:52.810
on how things go.

00:19:52.810,00:19:59.200
Then. number three in the seven principles
from Harvard is. work on the hard parts.

00:19:59.200,00:20:10.050
Which is kind of like this idea of practice.
deliberate practice.

00:20:10.050,00:20:19.490
Work on the hard parts means that you don't
just swing a bat at a ball every time. you

00:20:19.490,00:20:22.340
know. you go out and just muck around.

00:20:22.340,00:20:27.459
You train properly. you find the bit that
you are the least good at. you figure out

00:20:27.460,00:20:31.340
where the problems are. you work damn hard
at it.

00:20:31.340,00:20:39.530
So. in the deep learning context. that means
that we do not dumb things down.

00:20:39.530,00:20:40.530
Right?

00:20:40.530,00:20:45.210
By the end of the course. you will have done
the calculus.

00:20:45.210,00:20:47.400
You will have done the linear algebra.

00:20:47.400,00:20:54.380
You will have done the software engineering
of the code. right?

00:20:54.380,00:21:04.290
You will be practicing these things which
are hard. so it requires tenacity and commitment.

00:21:04.290,00:21:11.290
But hopefully. you'll understand why it matters
because before you start practicing something

00:21:11.290,00:21:14.470
you'll know why you need that thing because
you'll be using it.

00:21:14.470,00:21:19.450
Like to make your model better. you'll have
to understand that concept first.

00:21:19.450,00:21:24.490
So for those of you used to a traditional
university environment. this is gonna feel

00:21:24.490,00:21:31.190
pretty weird and a lot of people say: “that
they regret (you know after a year of studying

00:21:31.190,00:21:37.560
fastai) that they spent too much time studying
theory. and not enough time training models

00:21:37.560,00:21:39.500
and writing code.

00:21:39.500,00:21:43.250
That's the kind of like. the number one piece
of feedback we get from people who say. “I

00:21:43.250,00:21:44.570
wish I've done things differently.”

00:21:44.570,00:21:45.790
It's that.

00:21:45.790,00:21:53.120
So please try to. as best as you can. since
you're here. follow along with this approach.

00:21:53.120,00:21:59.399
We are gonna be using a software stack - Sorry
Rachel.

00:21:59.400,00:22:00.400
Yes?

00:22:00.400,00:22:02.510
I just need to say one more thing about the
approach.

00:22:02.510,00:22:07.390
I think since. so many of us spent so many
years with the traditional educational approach

00:22:07.390,00:22:12.030
of bottom-up. that this can feel very uncomfortable
at first.

00:22:12.030,00:22:16.620
I still feel uncomfortable with it sometimes.
even though I'm committed to the idea.

00:22:16.620,00:22:21.850
And that. some of it is also having to catch
yourself and being okay with not knowing the

00:22:21.850,00:22:22.850
details.

00:22:22.850,00:22:28.189
Which I think can feel very unfamiliar. or
even wrong when you're kind of new to that.

00:22:28.190,00:22:31.930
Of like: “Oh wait. I'm using something and
I don't understand every underlying detail.”

00:22:31.930,00:22:36.370
But you kind of have to trust that we're gonna
get to those details later.

00:22:36.370,00:22:39.560
So I can't empathise because I did not spend
lots of time doing that.

00:22:39.560,00:22:44.050
But I will tell you this - teaching this way
is very. very. very hard.

00:22:44.050,00:22:49.430
And I very often find myself jumping back
into a foundations first approach.

00:22:49.430,00:22:51.810
Because it's just so easy to be like: “Oh
you need to know this.

00:22:51.810,00:22:52.810
You need to know this.

00:22:52.810,00:22:53.810
You need to do this.

00:22:53.810,00:22:55.139
And then you can know this.”

00:22:55.140,00:22:56.540
That's so much easier to teach.

00:22:56.540,00:23:01.760
So I do find this much much more challenging
to teach. but hopefully it's worth it.

00:23:01.760,00:23:06.980
We spent a long long time figuring out how
to get deep learning into this format.

00:23:06.980,00:23:11.620
But one of the things that helps us here.
is the software we have available.

00:23:11.620,00:23:23.010
If you haven't used Python before - it's ridiculously
flexible and expressive and easy-to-use language.

00:23:23.010,00:23:28.210
We have plenty of bits about it we don't love
but on the whole we love the overall thing.

00:23:28.210,00:23:33.790
And we think it's - Most importantly. the
vast. vast. vast majority of deep learning

00:23:33.790,00:23:38.000
practitioners and researchers are using Python.

00:23:38.000,00:23:42.680
On top of Python. there are two libraries
that most folks are using today; PyTorch and

00:23:42.680,00:23:44.140
TensorFlow.

00:23:44.140,00:23:47.550
There's been a very rapid change here.

00:23:47.550,00:23:51.090
TensorFlow was what we were teaching until
a couple of years ago.

00:23:51.090,00:23:54.970
It's what everyone was using until a couple
of years ago.

00:23:54.970,00:24:00.370
It got super bogged down. basically TensorFlow
got super bogged down.

00:24:00.370,00:24:05.929
This other software called PyTorch came along
that was much easier to use and much more

00:24:05.930,00:24:15.040
useful for researchers and within the last
12 months. the percentage of papers at major

00:24:15.040,00:24:21.290
conferences that uses PyTorch has gone from
20% to 80% and vice versa. those that use

00:24:21.290,00:24:24.960
TensorFlow have gone on from 80% to 20%.

00:24:24.960,00:24:29.030
So basically all the folks that are actually
building the technology were all using. are

00:24:29.030,00:24:35.340
now using. PyTorch and you know industry moves
a bit more slowly but in the next year or

00:24:35.340,00:24:38.899
two you will probably see a similar thing
in industry.

00:24:38.900,00:24:45.180
Now. the thing about PyTorch is it's super
super flexible and really is designed for

00:24:45.180,00:24:52.570
flexibility and developer friendliness. certainly
not designed for beginner friendliness and

00:24:52.570,00:24:57.950
it's not designed for what we say. it doesn't
have higher level API's. by which I mean there

00:24:57.950,00:25:05.630
isn't really things to make it easy to build
stuff quickly using PyTorch.

00:25:05.630,00:25:13.700
So to deal with that issue. we have a library
called fastai that sits on top of PyTorch.

00:25:13.700,00:25:20.330
Fastai is the most popular higher level API
for PyTorch.

00:25:20.330,00:25:27.520
It is. because our courses are so popular.
some people are under the mistaken impression

00:25:27.520,00:25:34.260
that fastai is designed for beginners or for
teaching.

00:25:34.260,00:25:43.840
It is designed for beginners and teaching.
as well as practitioners in industry and researchers.

00:25:43.840,00:25:50.290
The way we do this makes sure that it's. that
it's the best API for all of those people

00:25:50.290,00:25:59.340
as we use something called a layered API and
so there's a peer-reviewed paper that Sylvain

00:25:59.340,00:26:04.600
and I wrote that described how we did that
and for those of you that are software engineers.

00:26:04.600,00:26:08.129
it will not be at all unusual or surprising.

00:26:08.130,00:26:12.590
It's just totally standard software engineering
practices. but they are practices that were

00:26:12.590,00:26:17.030
not followed in any deep learning library
we had seen.

00:26:17.030,00:26:24.500
Just basically lots of re-factoring and decoupling
and so by using that approach. it's allowed

00:26:24.500,00:26:32.410
us to build something which you can do super
low-level research. you can do state-of-the-art

00:26:32.410,00:26:43.850
production models and you can do kind of super
easy. beginner. but beginner world-class models.

00:26:43.850,00:26:47.219
So that's the basic software stack. there's
other pieces of software we will be learning

00:26:47.220,00:26:49.940
about along the way.

00:26:49.940,00:26:54.570
But the main thing I think to mention here
is it actually doesn't matter.

00:26:54.570,00:27:01.300
If you learn this software stack and then
at work you need to use TensorFlow and Keras.

00:27:01.300,00:27:06.379
you will be able to switch in less than a
week.

00:27:06.380,00:27:13.070
Lots and lots of students have done that.
it's never been a problem.

00:27:13.070,00:27:20.700
The important thing is to learn the concepts
and so we're going to focus on those concepts

00:27:20.700,00:27:27.870
and by using an API which minimizes the amount
of boilerplate you have to use. it means you

00:27:27.870,00:27:29.729
can focus on the bits that are important.

00:27:29.730,00:27:38.150
The actual lines of code will correspond much
more to the actual concepts you are implementing.

00:27:38.150,00:27:41.450
You are going to need a GPU machine.

00:27:41.450,00:27:49.420
A GPU is a Graphics Processing Unit. and specifically.
you need an Nvidia GPU.

00:27:49.420,00:27:55.520
Other brands of GPU just aren't well supported
by any Deep Learning libraries.

00:27:55.520,00:27:56.970
Please don't buy one.

00:27:56.970,00:28:00.130
If you already have one you probably shouldn't
use it.

00:28:00.130,00:28:05.400
Instead you should use one of the platforms
that we have already got set up for you.

00:28:05.400,00:28:10.410
It's just a huge distraction to be spending
your time doing. like. system administration

00:28:10.410,00:28:16.330
on a GPU machine and installing drivers and
blah blah blah.

00:28:16.330,00:28:18.000
And run it on Linux.

00:28:18.000,00:28:19.000
Please.

00:28:19.000,00:28:22.260
That's what everybody's doing. not just us.
everybody's running it on Linux.

00:28:22.260,00:28:23.370
Make life easy for yourself.

00:28:23.370,00:28:28.199
It's hard enough to learn Deep Learning without
having to do it in a way that you are learning.

00:28:28.200,00:28:31.550
you know. all kinds of arcane hardware support
issues.

00:28:31.550,00:28:43.480
There's a lot of free options available and
so. please. please use them.

00:28:43.480,00:28:48.280
If you're using an option that is not free
don't forget to shut down your instance.

00:28:48.280,00:28:51.730
So what's gonna be happening is you gonna
be spinning up a server that lives somewhere

00:28:51.730,00:28:57.260
else in the world. and you're gonna be connecting
to it from your computer and training and

00:28:57.260,00:29:01.500
running and building models.

00:29:01.500,00:29:05.980
Just because you close your browser window
doesn't mean your server stops running on

00:29:05.980,00:29:06.980
the whole.

00:29:06.980,00:29:07.980
Right?

00:29:07.980,00:29:11.330
So don't forget to shut it down because otherwise
you're paying for it.

00:29:11.330,00:29:16.120
Colab. is a great system which is free.

00:29:16.120,00:29:18.649
There's also a paid subscription version of
it.

00:29:18.650,00:29:21.200
Be careful with Colab.

00:29:21.200,00:29:26.590
Most of the other systems we recommend save
your work for you automatically and you can

00:29:26.590,00:29:28.459
come back to it at any time.

00:29:28.460,00:29:29.460
Colab doesn't.

00:29:29.460,00:29:37.110
So be sure to check out the Colab platform
thread on the forums. to learn about that.

00:29:37.110,00:29:44.020
So. I mention the forums...

00:29:44.020,00:29:51.970
The forums are really. really important because
that is where all of the discussion and set

00:29:51.970,00:29:54.030
up and everything happens.

00:29:54.030,00:29:56.240
So for example if you want help with setup
here.

00:29:56.240,00:30:03.820
You know there is a setup help thread and
you can find out. you know. how to best set

00:30:03.820,00:30:09.149
up Colab. and you can see discussions about
it and you can ask questions. and please remember

00:30:09.150,00:30:12.310
to search before you ask your question. right?

00:30:12.310,00:30:18.620
Because it's probably been asked before. unless
you're one of the very. very earliest people

00:30:18.620,00:30:22.530
who are doing the course.

00:30:22.530,00:30:24.820
So. once you…

00:30:24.820,00:30:31.570
So. step one is to get your server set up
by just following the instructions from the

00:30:31.570,00:30:33.620
forums or from the course website.

00:30:33.620,00:30:39.879
And the course website will have lots of step-by-step
instructions for each platform.

00:30:39.880,00:30:44.680
They will vary in price. they will vary in
speed. they will vary in availability. and

00:30:44.680,00:30:46.390
so forth.

00:30:46.390,00:30:48.390
Once you are finished following those instructions

00:30:48.390,00:30:57.790
The last step of those instructions will end
up showing you something like this: a course

00:30:57.790,00:31:01.290
v4 folder. so a version four of our course.

00:31:01.290,00:31:04.820
By the time you see this video. this is likely
to have more stuff in it. but it will have

00:31:04.820,00:31:07.669
an NB's standing for notebooks folder.

00:31:07.670,00:31:15.260
So you can click on that. and that will show
you all of the notebooks for the course.

00:31:15.260,00:31:21.060
What I want you to do is scroll bottom and
find the one called app Jupyter.

00:31:21.060,00:31:27.760
Click on that. and this is where you can start
learning about Jupyter notebook.

00:31:27.760,00:31:30.220
What's Jupyter Notebook?

00:31:30.220,00:31:39.630
Jupyter Notebook is something where you can
start typing things. and press Shift-Enter.

00:31:39.630,00:31:41.250
and it will give you an answer.

00:31:41.250,00:31:47.440
And so the thing you're typing is python code.
and the thing that comes out is a result of

00:31:47.440,00:31:48.780
that code.

00:31:48.780,00:31:52.490
And so you can put in anything in python.

00:31:52.490,00:31:56.620
X equals three times four.

00:31:56.620,00:32:05.239
X plus one. and as you can see. it displays
a result anytime there's a result to display.

00:32:05.240,00:32:10.290
So for those of you that have done a bit of
coding before. you will recognise this as

00:32:10.290,00:32:11.490
a REPL.

00:32:11.490,00:32:16.370
R-E-P-L. read. evaluate. print. loop.

00:32:16.370,00:32:18.939
Most languages have some kind of REPL.

00:32:18.940,00:32:29.320
The Jupyter notebook REPL is particularly
interesting. because it has things like headings.

00:32:29.320,00:32:34.850
graphical outputs. interactive multimedia.

00:32:34.850,00:32:37.679
It's a really astonishing piece of software.

00:32:37.680,00:32:39.850
It's won some really big awards.

00:32:39.850,00:32:48.199
I would have thought the most widely used
REPL. outside of shells like bash.

00:32:48.200,00:32:50.280
It's a very powerful system.

00:32:50.280,00:32:51.330
We love it.

00:32:51.330,00:32:55.939
We've written our whole book in it. we've
written the entire FASTAI library with it.

00:32:55.940,00:32:58.970
we do all our teaching with it.

00:32:58.970,00:33:06.780
It's extremely unfamiliar to people who have
done most of their work in IDE.

00:33:06.780,00:33:11.070
You should expect it to feel as awkward as
perhaps the first time you moved from a GUI

00:33:11.070,00:33:13.200
to a command line.

00:33:13.200,00:33:14.200
It's different.

00:33:14.200,00:33:22.080
So if you're not familiar with REPL-based
systems. it's gonna feel super weird.

00:33:22.080,00:33:25.169
But stick with it. because it really is great.

00:33:25.170,00:33:31.980
The kind of model going on here is that. this
webpage I'm looking at. is letting me type

00:33:31.980,00:33:37.460
in things for a server to do. and show me
the results of computations a server is doing.

00:33:37.460,00:33:40.170
So the server is off somewhere else.

00:33:40.170,00:33:42.650
It's not running on my computer right?

00:33:42.650,00:33:45.810
The only thing running on the computer is
this webpage.

00:33:45.810,00:33:53.360
But as I do things. so for example if I say
X equals X times three.

00:33:53.360,00:33:55.929
This is updating the servers state.

00:33:55.930,00:33:56.950
There's this state.

00:33:56.950,00:34:02.530
It's like what's currently the value of X
and so I can find out. now X is something

00:34:02.530,00:34:03.530
different.

00:34:03.530,00:34:09.668
So you can see. when I did this line here.
it didn't change the earlier X plus one. right?

00:34:09.668,00:34:14.730
So that means that when you look at a Jupyter
notebook. it's not showing you the current

00:34:14.730,00:34:16.559
state of your server.

00:34:16.559,00:34:21.679
It's just showing you what that state was.
at the time that you printed that thing out.

00:34:21.679,00:34:24.889
It's just like if you use a shell like bash.

00:34:24.889,00:34:26.780
And you type “ls”.

00:34:26.780,00:34:28.629
And then you delete a file.

00:34:28.629,00:34:31.989
That earlier “ls” you printed doesn't
go back and change.

00:34:31.989,00:34:35.558
That's kind of how REPLs generally work.

00:34:35.559,00:34:38.679
Including this one.

00:34:38.679,00:34:43.280
Jupyter notebook has two modes.

00:34:43.280,00:34:48.880
One is edit mode. which is when I click on
a cell and I get a flashing cursor and I can

00:34:48.880,00:34:52.190
move left and right and type.

00:34:52.190,00:34:53.389
Right?

00:34:53.389,00:34:55.740
There's not very many keyboard shortcuts to
this mode.

00:34:55.739,00:35:01.609
One useful one is “control” or “command”
+ “/”. Which will comment and uncomment.

00:35:01.609,00:35:07.509
The main one to know is “shift” + “enter”
to actually run the cell.

00:35:07.509,00:35:09.950
At that point there is no flashing cursor
anymore.

00:35:09.950,00:35:12.319
And that means that I'm now in command mode.

00:35:12.319,00:35:13.640
Not edit mode.

00:35:13.640,00:35:17.460
So as I go up and down. I'm selecting different
cells.

00:35:17.460,00:35:23.480
So in command mode as we move around. we're
now selecting cells.

00:35:23.480,00:35:26.829
And there are now lots of keyboard shortcuts
you can use.

00:35:26.829,00:35:30.600
So if you hit “H” you can get a list of
them.

00:35:30.600,00:35:36.150
That. for example - And you'll see that they're
not on the whole. like “control” or “command”

00:35:36.150,00:35:38.049
with something they're just the letter on
its own.

00:35:38.049,00:35:41.799
So if you use like Vim. you'll be more familiar
with this idea.

00:35:41.799,00:35:45.680
So for example if I hit “C” to copy and
“V” to paste.

00:35:45.680,00:35:47.440
Then it copies the cell.

00:35:47.440,00:35:50.769
Or “X” to cut it.

00:35:50.769,00:35:55.149
“A” to add a new cell above.

00:35:55.150,00:35:58.780
And then I can press the various number keys.
to create a heading.

00:35:58.780,00:36:01.890
So number two will create a heading level
II.

00:36:01.890,00:36:08.319
And as you can see. I can actually type formatted
text not just code.

00:36:08.319,00:36:15.569
The formatted text I type is in Markdown.

00:36:15.569,00:36:22.990
Like so.

00:36:22.990,00:36:24.368
My numbered one work.

00:36:24.369,00:36:26.800
There you go.

00:36:26.800,00:36:28.880
So that's in Markdown.

00:36:28.880,00:36:34.950
If you haven't used Markdown before. it's
a super super useful way to write formatted

00:36:34.950,00:36:35.950
text.

00:36:35.950,00:36:38.359
That is used very. very. very widely.

00:36:38.359,00:36:41.759
So learn it because it's super handy.

00:36:41.760,00:36:46.390
And you need it for Jupiter.

00:36:46.390,00:36:51.930
So when you look at our book notebooks.

00:36:51.930,00:36:57.839
For example. you can see an example of all
the kinds of formatting and code and stuff

00:36:57.839,00:36:58.839
here.

00:36:58.839,00:37:05.240
So you should go ahead and go through the
“app_jupyter”.

00:37:05.240,00:37:08.868
And you can see here how you can create plots
for example.

00:37:08.869,00:37:10.900
And create lists of things.

00:37:10.900,00:37:12.869
And import libraries.

00:37:12.869,00:37:18.430
And display pictures and so forth.

00:37:18.430,00:37:25.740
If you wanna create a new notebook. you can
just go “New” “Python 3” and that

00:37:25.740,00:37:29.939
creates a new notebook.

00:37:29.940,00:37:35.450
Which by default is just called “Untitled”
so you can then rename it to give it whatever

00:37:35.450,00:37:38.460
name you like.

00:37:38.460,00:37:45.480
And so then you'll now see that. in the list
here. “newname”.

00:37:45.480,00:37:50.269
The other thing to know about Jupiter is that
it's a nice easy way to jump into a terminal

00:37:50.269,00:37:51.450
if you know how to use a terminal.

00:37:51.450,00:37:54.509
You certainly don't have to for this course
at least for the first bit.

00:37:54.509,00:38:08.269
If I go to a new terminal. You can see here
I have a terminal.

00:38:08.269,00:38:19.140
One thing to note is for the notebooks are
attached to a Github repository.

00:38:19.140,00:38:21.950
If you haven't used Github before that's fine.

00:38:21.950,00:38:27.290
but basically they're attached to a server
where from time to time we will update the

00:38:27.290,00:38:29.480
notebooks on it.

00:38:29.480,00:38:33.329
And we will see you'll see on the course website.
in the forum we tell you how to make sure

00:38:33.329,00:38:35.890
you have the most recent versions.

00:38:35.890,00:38:40.640
When you grab our most recent version you
don't want to conflict with or overwrite your

00:38:40.640,00:38:41.640
changes.

00:38:41.640,00:38:50.328
So as you start experimenting it's not a bad
idea to like select a notebook and click duplicate

00:38:50.329,00:38:52.500
and then start doing your work in the copy.

00:38:52.500,00:38:59.099
And that way when you get an update of our
latest course materials. it's not gonna interfere

00:38:59.099,00:39:06.089
with the experiments you've been running.

00:39:06.089,00:39:09.490
So there are two important repositories to
know about.

00:39:09.490,00:39:20.089
One is the fast book repository which we saw
earlier. which is kind of the full book with

00:39:20.089,00:39:26.099
all the outputs and pros and everything.

00:39:26.099,00:39:30.390
And then the other one is the course V4 repository.

00:39:30.390,00:39:34.690
And here is the exact same notebook from the
course V4 repository.

00:39:34.690,00:39:42.130
And for this one we remove all of the pros
and all of the pictures and all of the outputs

00:39:42.130,00:39:46.210
and just leave behind the headings and the
code.

00:39:46.210,00:39:51.029
In this case you can see some outputs because
I just ran that code. most of it.

00:39:51.029,00:39:53.410
There won't be any.

00:39:53.410,00:39:56.299
No. No. I guess we have left outputs.

00:39:56.300,00:39:58.009
I'm not sure to keep that or not.

00:39:58.009,00:40:01.849
So you may or may not see the outputs.

00:40:01.849,00:40:04.259
So the idea with this is.

00:40:04.259,00:40:10.480
This is properly the version that you want
to be experimenting with.

00:40:10.480,00:40:15.170
Because it kind of forces you to think about
like what's going on as you do each step.

00:40:15.170,00:40:18.799
rather than just reading it and running it
without thinking.

00:40:18.799,00:40:24.059
We kind of want you to do it in a small bare
environment in which you thinking about like

00:40:24.059,00:40:28.859
what did the book say why was this happening
and if you forget anything then you kind of

00:40:28.859,00:40:31.321
go back to the book.

00:40:31.321,00:40:36.910
The other thing to mention is both the course
V4 version and the fast book version at the

00:40:36.910,00:40:42.000
end have a questionnaire.

00:40:42.000,00:40:46.529
And a quite a few folks have told us you know
that in amongst the reviewers and stuff that

00:40:46.529,00:40:49.990
they actually read the questionnaire first.

00:40:49.990,00:40:57.868
We spent many. many weeks writing the questionnaires.
Sylvain and I.

00:40:57.869,00:41:04.380
And the reason for that is because we try
to think about like what do we want you to

00:41:04.380,00:41:07.680
take away from each notebook.

00:41:07.680,00:41:10.029
So you kind of read the questionnaire first.

00:41:10.029,00:41:12.150
You can find out what are the things we think
are important.

00:41:12.150,00:41:14.940
What are the things you should know before
you move on.

00:41:14.940,00:41:18.859
So rather than having like a summary section
at the end saying at the end of this you should

00:41:18.859,00:41:24.920
know. blah blah blah. we instead have a questionnaire
to do the same thing. so please make sure

00:41:24.920,00:41:27.730
you do the questionnaire before you move onto
the next chapter.

00:41:27.730,00:41:31.600
You don't have to get everything right. and
most of the time answering the questions is

00:41:31.600,00:41:37.999
as simple as going back to that part of the
notebook and reading the prose. but if you've

00:41:37.999,00:41:43.288
missed something. like do go back and read
it because these are the things we are assuming

00:41:43.289,00:41:44.960
you know.

00:41:44.960,00:41:50.420
So if you don't know these things before you
move on. it could get frustrating.

00:41:50.420,00:41:57.499
Having said that. if you get stuck after trying
a couple of times. do move onto the next chapter.

00:41:57.499,00:42:00.808
do two or three more chapters and then come
back.

00:42:00.809,00:42:04.480
maybe by the time you've done a couple more
chapters. you know. you will get some more

00:42:04.480,00:42:05.480
perspective.

00:42:05.480,00:42:13.099
We try to re-explain things multiple times
in different ways. so it's okay if you tried

00:42:13.099,00:42:17.329
and you get stuck. then you can try moving
on.

00:42:17.329,00:42:26.410
Alright. so. let's try running the first part
of the notebook.

00:42:26.410,00:42:37.490
So here we are in 01 intro. so this is chapter
1 and here is our first cell.

00:42:37.490,00:42:45.481
So I click on the cell and by default. actually.
there will be a header in the toolbar as you

00:42:45.481,00:42:46.481
can see.

00:42:46.481,00:42:47.481
You can turn them on or off.

00:42:47.481,00:42:53.640
I always leave them off myself and so to run
this cell. you can either click on the play.

00:42:53.640,00:42:56.940
the run button or as I mentioned. you can
hit shift enter.

00:42:56.940,00:43:03.349
So for this one this i'll just click and as
you can see this star appears. so this says

00:43:03.349,00:43:07.880
I'm running and now you can see this progress
bar popping up and that is going to take a

00:43:07.880,00:43:15.680
few seconds and so as it runs to print out
some results.

00:43:15.680,00:43:20.740
Don't expect to get exactly the same results
as us. there is some randomness involved in

00:43:20.740,00:43:23.779
training a model. and that's okay.

00:43:23.779,00:43:26.530
Don't expect to get exactly the same time
as us.

00:43:26.530,00:43:32.510
If this first cell takes more than five minutes
unless you have a really old GPU that is probably

00:43:32.510,00:43:33.510
a bad sign.

00:43:33.510,00:43:38.609
You might want to hop on the forums and figure
out what's going wrong or maybe it only has

00:43:38.609,00:43:43.400
windows which really doesn't work very well
for this moment.

00:43:43.400,00:43:45.210
Don't worry that we don't know what all the
code does yet.

00:43:45.210,00:43:52.410
We are just making sure that we can train
a model . So here we are. it's finished running

00:43:52.410,00:43:58.078
and so as you can see. it's printed out some
information and in this case it's showing

00:43:58.079,00:44:05.269
me that there is an error rate of 0.005 at
doing something.

00:44:05.269,00:44:06.569
What is the something it's doing?

00:44:06.569,00:44:14.089
Well. what it's doing here is it's actually
grabbing a dataset. we call the pets dataset.

00:44:14.089,00:44:18.849
which is a dataset of pictures of cats and
dogs.

00:44:18.849,00:44:25.829
And it's trying to figure out; which ones
are cats and which ones are dogs.

00:44:25.829,00:44:32.599
And as you can see. after about well less
than a minute. it's able to do that with a

00:44:32.599,00:44:34.809
0.5% error rate.

00:44:34.809,00:44:37.039
So it can do it pretty much perfectly.

00:44:37.039,00:44:39.509
So we've trained our first model.

00:44:39.509,00:44:40.539
We have no idea how.

00:44:40.539,00:44:41.799
We don't know what we were doing.

00:44:41.799,00:44:44.259
But we have indeed trained our model.

00:44:44.259,00:44:46.559
So that's a good start.

00:44:46.559,00:44:51.309
And as you can see. we can train models pretty
quickly on a single computer.

00:44:51.309,00:44:55.089
Which you know - Many of which you can get
for free.

00:44:55.089,00:45:00.869
One more thing to mention is. if you have
a Mac - doesn't matter whether you have Windows

00:45:00.869,00:45:05.069
or Mac or Linux in terms of what's running
in the browser.

00:45:05.069,00:45:11.470
But if you have a Mac. please don't try to
use that GPU.

00:45:11.470,00:45:16.149
Mac's actually - Apple doesn't even support
Nvidia GPUs anymore.

00:45:16.150,00:45:19.470
So that's really not gonna be a great option.

00:45:19.470,00:45:20.868
So stick with Linux.

00:45:20.869,00:45:25.010
It will make life much easier for you.

00:45:25.010,00:45:30.420
Right. actually the first thing we should
do is actually try it out.

00:45:30.420,00:45:34.750
So if - I claim we've trained a model that
can pick cats from dogs.

00:45:34.750,00:45:37.640
Let's make sure we can.

00:45:37.640,00:45:41.859
So let's - Check out this cell.

00:45:41.859,00:45:42.859
This is interesting.

00:45:42.859,00:45:43.859
Right?

00:45:43.859,00:45:47.940
We've created a widgets dot file upload object
and displayed it.

00:45:47.940,00:45:50.690
And this is actually showing us a clickable
button.

00:45:50.690,00:45:52.619
So as I mentioned this is an unusual REPL.

00:45:52.619,00:45:55.319
We can even create GUIs. in this REPL.

00:45:55.319,00:45:58.359
So if I click on this file upload.

00:45:58.359,00:46:00.170
And I can pick “cat”.

00:46:00.170,00:46:04.130
There we go.

00:46:04.130,00:46:11.230
And I can now turn that uploaded data into
an image.

00:46:11.230,00:46:14.319
There's a cat.

00:46:14.319,00:46:22.509
And now I can do predict. and it's a cat.

00:46:22.510,00:46:26.400
With a 99.96% probability.

00:46:26.400,00:46:29.910
So we can see we have just uploaded an image
that we've picked out.

00:46:29.910,00:46:30.930
So you should try this.

00:46:30.930,00:46:31.930
Right?

00:46:31.930,00:46:32.930
Grab a picture of a cat.

00:46:32.930,00:46:35.578
Find one from the Internet or go and take
a picture of one yourself.

00:46:35.579,00:46:38.910
And make sure that you get a picture of a
cat.

00:46:38.910,00:46:43.520
This is something which can recognise photos
of cats. not line drawings of cats.

00:46:43.520,00:46:46.940
And so as we'll see. in this course.

00:46:46.940,00:46:52.050
These kinds of models can only learn from
the kinds of information you give it.

00:46:52.050,00:46:57.130
And so far we've only given it. as you'll
discover. photos of cats.

00:46:57.130,00:47:06.700
Not anime cats. not drawn cats. not abstract
representations of cats but just photos.

00:47:06.700,00:47:11.470
So we're now gonna look at; what's actually
happened here?

00:47:11.470,00:47:15.930
And you'll see at the moment. I am not getting
some great information here.

00:47:15.930,00:47:26.259
If you see this. in your notebooks. you'll
have to go: file. trust notebook.

00:47:26.259,00:47:30.559
And that just tells Jupiter that it's allowed
to run the code necessary to display things.

00:47:30.559,00:47:33.509
to make sure there isn't any security problems.

00:47:33.509,00:47:35.880
And so you'll now see the outputs.

00:47:35.880,00:47:39.880
Sometimes you'll actually see some weird code
like this.

00:47:39.880,00:47:43.609
This is code that actually creates outputs.

00:47:43.609,00:47:46.348
So sometimes we hide that code.

00:47:46.349,00:47:47.800
Sometimes we show it.

00:47:47.800,00:47:51.940
So generally speaking. you can just ignore
the stuff like that and focus on what comes

00:47:51.940,00:47:52.940
out.

00:47:52.940,00:47:54.300
So I'm not gonna go through these.

00:47:54.300,00:48:00.660
Instead I'm gonna have a look at it - same
thing over here on the slides.

00:48:00.660,00:48:04.710
So what we're doing here is; we're doing machine
learning.

00:48:04.710,00:48:07.750
Deep learning is a kind of machine learning.

00:48:07.750,00:48:09.170
What is machine learning?

00:48:09.170,00:48:16.070
Machine learning is. just like regular programming.
it's a way to get computers to do something.

00:48:16.070,00:48:22.250
But in this case. it's pretty hard to understand
how you would use regular programming to recognise

00:48:22.250,00:48:24.000
dog photos from cat photos.

00:48:24.000,00:48:28.319
How do you kind of create the loops and the
variable assignments and the conditionals

00:48:28.319,00:48:31.788
to create a program that recognises dogs vs
cats in photos.

00:48:31.789,00:48:33.190
It's super hard.

00:48:33.190,00:48:34.589
Super super hard.

00:48:34.589,00:48:41.420
So hard. that until kind of the deep learning
era. nobody really had a model that was remotely

00:48:41.420,00:48:43.910
accurate at this apparently easy task.

00:48:43.910,00:48:46.970
Because we can't write down the steps necessary.

00:48:46.970,00:48:51.368
So normally. you know. we write down a function
that takes some inputs and goes through our

00:48:51.369,00:48:52.369
program.

00:48:52.369,00:48:55.569
Produces some results.

00:48:55.569,00:49:02.529
So this general idea where the program is
something that we write (the steps).

00:49:02.530,00:49:06.970
Doesn't seem to work great for things like
recognising pictures.

00:49:06.970,00:49:12.470
So back in 1949. somebody named Arthur Samuel
started trying to figure out a way to solve

00:49:12.470,00:49:16.029
problems like recognising pictures of cats
and dogs.

00:49:16.030,00:49:23.000
And in 1962. he described a way of doing this.

00:49:23.000,00:49:26.270
Well first of all he described the problem:
“Programming a computer for these kinds

00:49:26.270,00:49:31.070
of computations is at best a difficult task.

00:49:31.070,00:49:37.589
Because of the need to spell out every minute
step of the process in exasperating detail.

00:49:37.589,00:49:42.290
Computers are giant morons which all of us
coders totally recognise.”

00:49:42.290,00:49:46.769
So he said. okay. let's not tell the computer
the exact steps. but let's give it examples

00:49:46.769,00:49:50.459
of a problem to solve and figure out how to
solve it itself.

00:49:50.460,00:49:56.269
And so. by 1961 he had built a checkers program
that had beaten the Connecticut state champion.

00:49:56.269,00:50:03.450
not by telling it the steps to take to play
checkers. but instead by doing this. which

00:50:03.450,00:50:09.990
is: “arrange for an automatic means of testing
the effectiveness of a weight assignment in

00:50:09.990,00:50:15.689
terms of actual performance and a mechanism
for altering the weight assignment so as to

00:50:15.690,00:50:19.019
maximise performance.”

00:50:19.019,00:50:21.680
This sentence is the key thing.

00:50:21.680,00:50:24.440
And it's a pretty tricky sentence so you can
spend some time on it.

00:50:24.440,00:50:32.200
The basic idea is this; instead of saying
inputs to a program and then outputs.

00:50:32.200,00:50:36.430
Let's have inputs to a - let's call the program
now model.

00:50:36.430,00:50:38.140
It is the same basic idea.

00:50:38.140,00:50:40.348
Inputs to a model and results.

00:50:40.349,00:50:43.760
And then we're gonna have a second thing called
weights.

00:50:43.760,00:50:50.569
And so the basic idea is that this model is
something that creates outputs based not only

00:50:50.569,00:50:58.410
on. for example. the state of a checkers board.
but also based on some set of weights or parameters

00:50:58.410,00:51:02.319
that describe how that model is going to work.

00:51:02.320,00:51:09.039
So the idea is. if we could. like. enumerate
all the possible ways of playing checkers.

00:51:09.039,00:51:14.130
and then kind of describe each of those ways
using some set of parameters or what Samuel

00:51:14.130,00:51:15.830
called weights.

00:51:15.830,00:51:21.690
Then if we had a way of checking how effective
a current weight assignment is in terms of

00:51:21.690,00:51:27.549
actual performance. in other words. does that
particular enumeration of a strategy for playing

00:51:27.549,00:51:33.140
checkers end up winning or losing games. and
then a way to alter the weight assignment

00:51:33.140,00:51:35.598
so as to maximise the performance.

00:51:35.599,00:51:40.710
So then oh let's try increasing or decreasing
each one of those weights one at a time to

00:51:40.710,00:51:45.190
find out if there is a slightly better way
of playing checkers and then do that lots

00:51:45.190,00:51:51.950
of lots of times then eventually such a procedure
could be made entirely automatic and then

00:51:51.950,00:51:58.029
the machine so programmed would learn from
its experience so this little paragraph is.

00:51:58.030,00:52:00.200
is the thing.

00:52:00.200,00:52:08.649
This is machine learning a way of creating
programs such that they learn. rather than

00:52:08.650,00:52:11.349
programmed.

00:52:11.349,00:52:16.930
So if we had such a thing. then we would basically
now have something that looks like this: you

00:52:16.930,00:52:22.549
have inputs and weights again going into a
model. creating results. i.e. you won or you

00:52:22.549,00:52:26.769
lost. and then a measurement of performance.

00:52:26.769,00:52:30.348
So remember that was this key step and then
the second key step is a way to update the

00:52:30.349,00:52:35.609
weights based on the measured performance
and then you could look through this process

00:52:35.609,00:52:43.450
and create a) train a machine learning model
so this is the abstract idea.

00:52:43.450,00:52:49.259
So after it ran for a while. right. it's come
up with a set of weights which it's pretty

00:52:49.260,00:52:55.089
good. right. we can now forget the way it
was trained and we have something that is

00:52:55.089,00:53:02.359
just like this. right. except the word program
is now replaced with the word model.

00:53:02.359,00:53:07.239
So a trained model can be used just like any
other computer program.

00:53:07.239,00:53:13.509
So the idea is we are building a computer
program not by putting up the steps necessary

00:53:13.509,00:53:19.619
to do the task. but by training it to learn
to do the task at the end of which it's just

00:53:19.619,00:53:26.759
another program and so this is what's called
inference right is using a trained model as

00:53:26.759,00:53:37.980
a program to do a task such as playing checkers
so machine learning is training programs developed

00:53:37.980,00:53:43.309
by allowing a computer to learn from its experience
rather than through manually coding.

00:53:43.309,00:53:53.640
Ok how would you do this for image recognition.
what is that model and that set of weights

00:53:53.640,00:53:59.700
such that as we vary them it could get better
and better at recognising cats versus dogs.

00:53:59.700,00:54:02.210
I mean for checkers

00:54:02.210,00:54:06.780
It's not too hard to imagine how you could
kind of enumerate. depending on different

00:54:06.780,00:54:11.289
kinds of “how far away the opponent's piece
is from your piece.” what should you do

00:54:11.289,00:54:12.289
in that situation.

00:54:12.289,00:54:16.079
How should you weigh defensive versus aggressive
strategies. blah blah blah.

00:54:16.079,00:54:20.410
Not at all obvious how you do that for image
recognition.

00:54:20.410,00:54:29.240
So what we really want. is some function in
here which is so flexible that there is a

00:54:29.240,00:54:33.209
set of weights that could cause it to do anything.

00:54:33.210,00:54:40.059
A real--like the world's most flexible possible
function--and turns out that there is such

00:54:40.059,00:54:41.059
a thing.

00:54:41.059,00:54:44.140
It's a neural network.

00:54:44.140,00:54:50.328
So we'll be describing exactly what that mathematical
function is in the coming lessons.

00:54:50.329,00:54:56.160
To use it. it actually doesn't really matter
what the mathematical function is.

00:54:56.160,00:55:03.631
It's a function which is. we say. “parameterised”
by some set of weights by which I mean. as

00:55:03.631,00:55:12.259
I give it a different set of weights it does
a different task. and it can actually do any

00:55:12.259,00:55:17.970
possible task: something called the universal
approximation theorem tells us that mathematically

00:55:17.970,00:55:26.319
provably. this functional form can solve any
problem that is solvable to any level of accuracy.

00:55:26.319,00:55:28.589
If you just find the right set of weights.

00:55:28.589,00:55:33.210
Which is kind of restating what we described
earlier in that. like. how do we deal with

00:55:33.210,00:55:39.700
Minsky (the Marvin Minsky) problem so neural
networks are so flexible that if you could

00:55:39.700,00:55:44.609
find the right set of weights they can solve
any problem including “Is this a car or

00:55:44.609,00:55:46.288
is this dog.”

00:55:46.289,00:55:51.130
So that means you need to focus your effort
on the process of training that is finding

00:55:51.130,00:55:57.010
good weights. good weight assignments to use
Samuel's terminology.

00:55:57.010,00:55:59.770
So how do you do that?

00:55:59.770,00:56:09.239
We want a completely general way to do this--to
update the weights based on some measure of

00:56:09.239,00:56:14.200
performance. such as how good is it at recognising
cats versus dogs.

00:56:14.200,00:56:16.839
And luckily it turns out such a thing exists!

00:56:16.839,00:56:21.538
And that thing is called stochastic gradient
descent (or SGD).

00:56:21.539,00:56:26.540
Again. we'll look at exactly how it works.
we'll build it ourselves from scratch. but

00:56:26.540,00:56:28.529
for now we don't have to worry about it.

00:56:28.529,00:56:34.210
I will tell you this. though. neither SGD
nor neural nets are at all mathematically

00:56:34.210,00:56:35.210
complex.

00:56:35.210,00:56:38.829
They nearly entirely are addition and multiplication.

00:56:38.829,00:56:45.109
The trick is it just a lot of them--like billions
of them--so many more than we can intuitively

00:56:45.109,00:56:46.109
grasp.

00:56:46.109,00:56:52.940
They can do extraordinarily powerful things.
but they're not rocket science at all.

00:56:52.940,00:56:58.609
They are not complex things. and we will see
exactly how they work.

00:56:58.609,00:57:03.049
So that's the Arthur Samuel version. right?

00:57:03.049,00:57:08.580
Nowadays we don't use quite the same terminology.
but we use exactly the same idea.

00:57:08.580,00:57:12.660
So that function that sits in the middle.

00:57:12.660,00:57:14.549
we call an architecture.

00:57:14.549,00:57:20.779
An architecture is the function that we're
adjusting the weights to get it to do something.

00:57:20.779,00:57:24.190
That's the architecture. that's the functional
form of the model.

00:57:24.190,00:57:28.849
Sometimes people say model to mean architecture.
so don't let that confuse you too much.

00:57:28.849,00:57:30.559
But. really the right word is architecture.

00:57:30.559,00:57:34.619
We don't call them weights; we call them parameters.

00:57:34.619,00:57:40.410
Weights has a specific meaning- it's quite
a particular kind of parameter.

00:57:40.410,00:57:46.609
The things that come out of the model. the
architecture with the parameters. we call

00:57:46.609,00:57:49.808
them predictions.

00:57:49.809,00:57:55.660
The predictions are based on two kinds of
inputs: independent variables that's the data.

00:57:55.660,00:58:03.308
like the pictures of the cats and dogs. and
dependent variables also known as labels.

00:58:03.309,00:58:07.400
which is like the thing saying “this is
a cat”. “this is a dog”. “this is

00:58:07.400,00:58:08.400
a cat”.

00:58:08.400,00:58:09.779
So. that's your inputs.

00:58:09.779,00:58:12.769
So. the results are predictions.

00:58:12.769,00:58:18.669
The measure of performance. to use Arthur
Samuel's word. is known as the loss.

00:58:18.670,00:58:24.020
So. the loss is being calculated from the
labels on the predictions and then there's

00:58:24.020,00:58:26.720
the update back to the parameters.

00:58:26.720,00:58:33.209
Okay. so. this is the same picture as we saw.
but just putting in the words that we use

00:58:33.210,00:58:34.210
today.

00:58:34.210,00:58:40.039
So. this picture- if you forget. if I say
these are the parameters of this used for

00:58:40.039,00:58:44.220
this architecture to create a model- you can
go back and remind yourself what they mean.

00:58:44.220,00:58:45.220
What are the parameters?

00:58:45.220,00:58:46.500
What are the predictions?

00:58:46.500,00:58:47.500
What is the loss?

00:58:47.500,00:58:53.970
Okay. the loss of some function that measures
the performance of the model in such a way

00:58:53.970,00:58:56.790
that we can update the parameters.

00:58:56.790,00:59:06.170
So. it's important to note that deep learning
and machine learning are not magic. right?

00:59:06.170,00:59:13.380
The model can only be created where you have
data showing you examples of the thing that

00:59:13.380,00:59:14.869
you're trying to learn about.

00:59:14.869,00:59:20.859
It can only learn to operate on the patterns
that you've seen in the input used to train

00:59:20.859,00:59:22.029
it. right?

00:59:22.030,00:59:27.109
So. if we don't have any line drawings of
cats and dogs. then there's never going to

00:59:27.109,00:59:32.519
be an update to the parameters that makes
the architecture and so the architect and

00:59:32.519,00:59:34.419
the parameters together is the model.

00:59:34.420,00:59:39.650
So. to say the model. that makes the model
better at predicting line drawings of cats

00:59:39.650,00:59:43.799
and dogs because they just. they never received
those weight updates because they never received

00:59:43.799,00:59:46.680
those inputs.

00:59:46.680,00:59:51.239
Notice also that this learning approach only
ever creates predictions.

00:59:51.239,00:59:54.319
It doesn't tell you what to do about it.

00:59:54.319,00:59:58.019
That's going to be very important when we
think about things like a recommendation system

00:59:58.019,01:00:01.229
of like “what product do we recommend to
somebody”?

01:00:01.230,01:00:04.950
Well. I don't know- we don't do that. right?

01:00:04.950,01:00:09.759
We can predict what somebody will say about
a product we've shown them. but we're not

01:00:09.759,01:00:11.140
creating actions.

01:00:11.140,01:00:12.310
We're creating predictions.

01:00:12.310,01:00:16.619
That's a super important difference to recognize.

01:00:16.619,01:00:22.470
It's not enough just to have examples of input
data like pictures of dogs and cats.

01:00:22.470,01:00:26.308
We can't do anything without labels.

01:00:26.309,01:00:31.359
And so very often. organisations say: “we
don't have enough data”.

01:00:31.359,01:00:35.150
Most of the time they mean: “we don't have
enough labelled data”.

01:00:35.150,01:00:39.549
Because if a company is trying to do something
with deep learning. often it's because they're

01:00:39.549,01:00:43.180
trying to automate or improve something they're
already doing.

01:00:43.180,01:00:48.419
Which means by definition they have data about
that thing or a way to capture data about

01:00:48.420,01:00:49.420
that thing.

01:00:49.420,01:00:50.420
Cus they're doing it.

01:00:50.420,01:00:51.420
Right?

01:00:51.420,01:00:55.089
But often the tricky part is labelling it.

01:00:55.089,01:00:57.569
So for example in medicine.

01:00:57.569,01:01:00.788
If you're trying to build a model for radiology.

01:01:00.789,01:01:05.579
You can almost certainly get lots of medical
images about just about anything you can think

01:01:05.579,01:01:06.579
of.

01:01:06.579,01:01:11.769
But it might be very hard to label them according
to malignancy of a tumour or according to

01:01:11.769,01:01:18.479
whether or not meningioma is present or whatever.
because these kinds of labels are not necessarily

01:01:18.480,01:01:24.289
captured in a structured way. at least in
the US medical system.

01:01:24.289,01:01:31.700
So that's an important distinction that really
impacts your kind of strategy.

01:01:31.700,01:01:39.000
So then a model. as we saw from the PDP book.
a model operates in an environment.

01:01:39.000,01:01:40.000
Right?

01:01:40.000,01:01:44.420
You roll it out and you do something with.

01:01:44.420,01:01:49.640
And so then. this piece of that kind of PDP
framework is super important.

01:01:49.640,01:01:50.640
Right?

01:01:50.640,01:01:53.769
You have a model that's actually doing something.

01:01:53.769,01:01:59.069
For example. you've built a predictive policing
model that predicts (doesn't recommend actions)

01:01:59.069,01:02:02.460
it predicts where an arrest might be made.

01:02:02.460,01:02:06.670
This is something a lot of jurisdictions in
the US are using.

01:02:06.670,01:02:10.809
Now it's predicting that. based on data and
based on labelled data.

01:02:10.809,01:02:20.779
And in this case it's actually gonna be using
(in the US) for example data where. I think.

01:02:20.779,01:02:25.099
depending on whether you're black or white.
black people in the US. I think. get arrested

01:02:25.099,01:02:31.480
something like seven times more often for
say marijuana possession than whites.

01:02:31.480,01:02:37.579
Even though the actual underlying amount of
marijuana use is about the same in the two

01:02:37.579,01:02:38.579
populations.

01:02:38.579,01:02:42.079
So if you start with biased data and you build
a predictive policing model.

01:02:42.079,01:02:49.430
Its prediction will say: “oh you will find
somebody you can arrest here” based on some

01:02:49.430,01:02:50.430
biased data.

01:02:50.430,01:02:56.279
So then. law enforcement officers might decide
to focus their police activity on the areas

01:02:56.279,01:02:58.039
where those predictions are happening.

01:02:58.039,01:03:01.940
As a result of which they'll find more people
to arrest.

01:03:01.940,01:03:05.430
And then they'll use that. to put it back
into the model.

01:03:05.430,01:03:09.788
Which will now find: “oh there's even more
people we should be arresting in the black

01:03:09.789,01:03:12.640
neighbourhoods” and thus it continues.

01:03:12.640,01:03:17.000
So this would be an example of how a model
interacting with its environment creates something

01:03:17.000,01:03:19.460
called a positive feedback loop.

01:03:19.460,01:03:23.930
Where the more a model is used. the more biased
the data becomes. making the model even more

01:03:23.930,01:03:26.440
biased and so forth.

01:03:26.440,01:03:32.299
So one of the things to be super careful about
with machine learning is; recognising how

01:03:32.299,01:03:38.009
that model is actually being used and what
kinds of things might happen as a result of

01:03:38.009,01:03:39.009
that.

01:03:39.009,01:03:48.910
I was just going to add that this is an example
of proxies because here arrest is being used

01:03:48.910,01:03:56.210
as a proxy for crime. and I think that pretty
much in all cases. the data that you actually

01:03:56.210,01:04:00.049
have is a proxy for some value that you truly
care about.

01:04:00.049,01:04:06.109
And that difference between the proxy and
the actual value often ends up being significant.

01:04:06.109,01:04:10.770
Thanks. Rachel.

01:04:10.770,01:04:13.430
That's a really important point.

01:04:13.430,01:04:24.219
Okay. so let's finish off by looking at what's
going on with this code.

01:04:24.219,01:04:34.880
So the code we ran is. basically -- one. two.
three. four. five. six -- lines of code.

01:04:34.880,01:04:39.829
So the first line of code is an import line.

01:04:39.829,01:04:46.690
So in Python you can't use an external library
until you import from it.

01:04:46.690,01:04:53.339
Normally in place. people import just the
functions and classes that they need from

01:04:53.339,01:04:55.029
the library.

01:04:55.030,01:05:01.989
But Python does provide a convenient facility
where you can import everything from a module.

01:05:01.989,01:05:04.410
which is by putting a start there.

01:05:04.410,01:05:07.029
Most of the time. this is a bad idea.

01:05:07.029,01:05:12.479
Because. by default. the way Python works
is that if you say import star. it doesn't

01:05:12.479,01:05:16.640
only import the things that are interesting
and important in the library you're trying

01:05:16.640,01:05:18.520
to get something from.

01:05:18.520,01:05:23.369
But it also imports things from all the libraries
it used. and all the libraries they used.

01:05:23.369,01:05:27.190
and you end up kind of exploding your namespace
in horrible ways and causing all kinds of

01:05:27.190,01:05:28.760
bugs.

01:05:28.760,01:05:36.510
Because fastai is designed to be used in this
REPL environment where you want to be able

01:05:36.510,01:05:41.779
to do a lot of quick rapid prototyping. we
actually spent a lot of time figuring out

01:05:41.779,01:05:45.410
how to avoid that problem so that you can
import star safely.

01:05:45.410,01:05:49.629
So. whether you do this or not. is entirely
up to you.

01:05:49.630,01:05:56.609
But rest assured that if you import star from
a fastai library. it's actually been explicitly

01:05:56.609,01:06:01.999
designed in a way that you only get the bits
that you actually need.

01:06:01.999,01:06:05.799
One thing to mention is in the video you see
it's called “fastai2.”

01:06:05.799,01:06:09.849
That's because we're recording this video
using a prerelease version.

01:06:09.849,01:06:19.049
By the time you are watching the online. the
MOOC. version of this. the 2 will be gone.

01:06:19.049,01:06:25.609
Something else to mention is. there are. as
I speak. four main predefined applications

01:06:25.609,01:06:31.098
in fastai. being vision. text. tabular and
collaborative filtering.

01:06:31.099,01:06:35.130
We'll be learning about all of them and a
lot more.

01:06:35.130,01:06:41.989
For each one. say here's vision. you can import
from the .all. kind of meta-model. I guess

01:06:41.989,01:06:42.989
we could call it.

01:06:42.989,01:06:48.079
And that will give you all the stuff that
you need for most common vision applications.

01:06:48.079,01:06:55.779
So. if you're using a REPL system like Jupyter
notebook. it's going to give you all the stuff

01:06:55.779,01:07:01.619
right there that you need without having to
go back and figure it out.

01:07:01.619,01:07:06.559
One of the issues with this is a lot of the
python users don't.

01:07:06.559,01:07:12.450
If they look at something like untar_data.
they would figure out where it comes from

01:07:12.450,01:07:14.218
by looking at the import line.

01:07:14.219,01:07:15.970
And so if you import star. you can't do that
anymore.

01:07:15.970,01:07:19.828
The good news. in a REPL. you don't have to.

01:07:19.829,01:07:27.450
You can literally just type the symbol. press
SHIFT - ENTER and it will tell you exactly

01:07:27.450,01:07:28.669
where it came from.

01:07:28.670,01:07:29.940
As you can see.

01:07:29.940,01:07:33.510
So that's super handy.

01:07:33.510,01:07:43.859
So in this case. for example. to do the actual
building of the dataset. we called ImageDataLoaders.from_name_func.

01:07:43.859,01:07:51.170
I can actually call the special doc function
to get the documentation for that.

01:07:51.170,01:07:57.349
As you can see. it tells me exactly everything
to pass in. what all the defaults are. and

01:07:57.349,01:08:07.940
most importantly. not only what it does. but
SHOW IN DOCS pops me over to the full documentation

01:08:07.940,01:08:11.190
including an example.

01:08:11.190,01:08:17.180
Everything in the fastAI documentation has
an example and the cool thing is: the entire

01:08:17.180,01:08:20.770
documentation is written in Jupyter Notebooks.

01:08:20.770,01:08:25.540
So that means you can actually open the Jupyter
Notebook for this documentation and run the

01:08:25.540,01:08:33.279
line of code yourself and see it actually
working and look at the outputs and so forth.

01:08:33.279,01:08:36.759
Also in the documentation. you'll find that
there are a bunch of tutorials.

01:08:36.760,01:08:40.501
For example. if you look at the vision tutorial.
it will cover lots of things but one of the

01:08:40.501,01:08:44.910
things we will cover is. as you can see in
this case. pretty much the same kind of stuff

01:08:44.910,01:08:47.710
we are actually looking at in Lesson 1.

01:08:47.710,01:08:53.300
So there is a lot of documentation in fastAI
and taking advantage of it is a pretty good

01:08:53.300,01:08:54.300
idea.

01:08:54.300,01:08:59.310
It is fully searchable and as I mentioned.
perhaps most importantly. every one of these

01:08:59.310,01:09:05.050
documentation pages is also a fully interactive
Jupyter Notebook.

01:09:05.050,01:09:13.839
So. looking through more of this code. the
first line after the import is something that

01:09:13.839,01:09:14.910
uses untar_ data.

01:09:14.910,01:09:20.019
That will download a dataset. decompress it.
and put it on your computer.

01:09:20.020,01:09:22.770
If it is already downloaded. it won't download
it again.

01:09:22.770,01:09:25.710
If it is already decompressed it won't decompress
it again.

01:09:25.710,01:09:32.540
And as you can see. fastAI already has predefined
access to a number of really useful datasets.

01:09:32.540,01:09:34.180
such as this PETS dataset.

01:09:34.180,01:09:39.920
Datasets are a super important part. as you
can imagine of deep learning.

01:09:39.920,01:09:41.899
We will be seeing lots of them.

01:09:41.899,01:09:47.689
And these are created by lots of heroes (and
heroines) who basically spend months or years

01:09:47.689,01:09:51.259
collating data that we can use to build these
models.

01:09:51.260,01:10:00.230
The next step is to tell fastAI what this
data is and we will be learning a lot about

01:10:00.230,01:10:01.230
that.

01:10:01.230,01:10:05.559
But in this case. we are basically saying.
‘okay. it contains images'.

01:10:05.560,01:10:07.630
It contains images that are in this path.

01:10:07.630,01:10:14.100
So untar_data returns the path that is whereabouts
it has been decompressed to.

01:10:14.100,01:10:18.900
Or if it is already decompressed. it tells
us where it was previously decompressed to.

01:10:18.900,01:10:23.559
We have to tell it things like ‘okay. what
images are actually in that path'.

01:10:23.560,01:10:27.170
One of the really interesting ones is label_func.

01:10:27.170,01:10:33.600
How do you tell. for each file. whether it
is a cat or a dog.

01:10:33.600,01:10:37.330
And if you actually look at the ReadME for
the original dataset. it uses a slightly quirky

01:10:37.330,01:10:42.430
thing which is they said. ‘oh. anything
where the first letter of the filename is

01:10:42.430,01:10:45.000
an uppercase is a cat'.

01:10:45.000,01:10:46.430
That's what they decided.

01:10:46.430,01:10:51.180
So we just created a little function here
called is_cat that returns the first letter.

01:10:51.180,01:10:52.480
is it uppercase or not.

01:10:52.480,01:10:57.419
And we tell fastai that's how you tell if
it's a cat.

01:10:57.420,01:11:01.260
We'll come back to these two in a moment.

01:11:01.260,01:11:04.640
So the next thing. now we've told it what
the data is.

01:11:04.640,01:11:06.350
We then have to create something called a
learner.

01:11:06.350,01:11:10.180
A learner is a thing that learns. it does
the training.

01:11:10.180,01:11:12.510
So you have to tell it what data to use.

01:11:12.510,01:11:16.570
Then you have to tell it what architecture
to use.

01:11:16.570,01:11:19.809
I'll be talking a lot about this in the course.

01:11:19.810,01:11:25.460
But. basically. there's a lot of predefined
neural network architectures that have certain

01:11:25.460,01:11:26.660
pros and cons.

01:11:26.660,01:11:30.360
And for computer vision. the architecture
is called ResNet.

01:11:30.360,01:11:34.580
Just a super great starting point. and so
we're just going to use a reasonably small

01:11:34.580,01:11:35.820
one of them.

01:11:35.820,01:11:39.710
So these are all predefined and set up for
you.

01:11:39.710,01:11:43.380
And then you can tell fastai what things you
want to print out as it's training.

01:11:43.380,01:11:47.730
And in this case. we're saying “oh. tell
us the error. please. as you train”.

01:11:47.730,01:11:51.230
So then we can call this really important
method called fine_tune that we'll be learning

01:11:51.230,01:11:57.049
about in the next lesson which actually does
the training.

01:11:57.050,01:12:00.700
valid_pct does something very important.

01:12:00.700,01:12:08.160
It grabs. in this case. 20% of the data (.2
proportion). and does not use it for training

01:12:08.160,01:12:09.230
a model.

01:12:09.230,01:12:12.730
Instead. it uses it for telling you the error
rate of the model.

01:12:12.730,01:12:19.889
So. always in fastai this metric. error_rate.
will always be calculated on a part of the

01:12:19.890,01:12:22.290
data which has not been trained with.

01:12:22.290,01:12:27.040
And the idea here. and we'll talk a lot about
more about this in future lessons.

01:12:27.040,01:12:31.220
But the basic idea here is we want to make
sure that we're not overfitting.

01:12:31.220,01:12:33.540
Let me explain.

01:12:33.540,01:12:35.090
Overfitting looks like this.

01:12:35.090,01:12:38.860
Let's say you're trying to create a function
that fits all these dots. right.

01:12:38.860,01:12:44.049
A nice function would look like that. right.

01:12:44.050,01:12:47.940
But you could also fit. you can actually fit
it much more precisely with this function.

01:12:47.940,01:12:50.969
Look. this is going much closer to all the
dots than this one is.

01:12:50.970,01:12:53.240
So. this is obviously a better function.

01:12:53.240,01:13:00.010
Except. as soon as you get outside where the
dots are. especially if you go off the edges.

01:13:00.010,01:13:01.640
it's obviously doesn't make any sense.

01:13:01.640,01:13:06.060
So. this is what you'd call an overfit function.

01:13:06.060,01:13:08.570
So. overfitting happens for all kinds of reasons.

01:13:08.570,01:13:11.969
We use a model that's too big or we use not
enough data.

01:13:11.970,01:13:14.510
We'll be talking all about it. right.

01:13:14.510,01:13:22.370
But. really the craft of deep learning is
all about creating a model that has a proper

01:13:22.370,01:13:23.370
fit.

01:13:23.370,01:13:27.480
And the only way you know if a model has a
proper fit is by seeing whether it works well

01:13:27.480,01:13:31.620
on data that was not used to train it.

01:13:31.620,01:13:36.830
And so. we always set aside some of the data
to create something called a validation set.

01:13:36.830,01:13:41.269
The validation set is the data that we use
not to touch it at all when we're training

01:13:41.270,01:13:49.230
a model. but we're only using it to figure
out whether the model's actually working or

01:13:49.230,01:13:51.099
not.

01:13:51.100,01:13:57.080
One thing that Sylvain mentioned in the book.
is that one of the interesting things about

01:13:57.080,01:14:04.500
studying fastai is you learn a lot of interesting
programming practices.

01:14:04.500,01:14:11.000
And so I've been programming. I mean. since
I was a kid. so like 40 years.

01:14:11.000,01:14:18.060
And Sylvain and I both work really. really
hard to make python do a lot of work for us

01:14:18.060,01:14:22.960
and to use. you know. programming practices
which make us very productive and allow us

01:14:22.960,01:14:27.120
to come back to our code. years later and
still understand it.

01:14:27.120,01:14:34.700
And so you'll see in our code we'll often
do things that you might not have seen before.

01:14:34.700,01:14:39.260
And so we. a lot of students who have gone
through previous courses say they learned

01:14:39.260,01:14:44.100
a lot about coding and python coding and software
engineering from the course.

01:14:44.100,01:14:49.350
So. yeah check. when you see something new.
check it out and feel free to ask on the forums

01:14:49.350,01:14:53.410
if you're curious about why something was
done that way.

01:14:53.410,01:14:59.250
One thing to mention is. just like I mentioned
import star is something most Python programmers

01:14:59.250,01:15:05.890
don't do cause most libraries don't support
doing it properly.

01:15:05.890,01:15:07.130
We do a lot of things like that.

01:15:07.130,01:15:11.240
We do a lot of things where we don't follow
a traditional approach to python programming.

01:15:11.240,01:15:19.940
Because I've used so many languages over the
years. I code not in a way that's specifically

01:15:19.940,01:15:23.940
pythonic. but incorporates like ideas from
lots of other languages and lots of other

01:15:23.940,01:15:31.230
notations and heavily customised our approach
to python programming based on what works

01:15:31.230,01:15:34.000
well for data science.

01:15:34.000,01:15:40.750
That means that the code you see in fastai
is not probably. not gonna fit with. the kind

01:15:40.750,01:15:45.600
of style guides and normal approaches at your
workplace. if you use Python there.

01:15:45.600,01:15:52.440
So. obviously. you should make sure that you
fit in with your organization's programming

01:15:52.440,01:15:56.419
practices rather than following ours.

01:15:56.420,01:16:00.910
But perhaps in your own hobby work. you can
follow ours and see if you find that are interesting

01:16:00.910,01:16:05.269
and helpful. or even experiment with that
in your company if you're a manager and you

01:16:05.270,01:16:08.480
are interested in doing so.

01:16:08.480,01:16:17.000
Okay. so to finish. I'm going to show you
something pretty interesting. which is. have

01:16:17.001,01:16:27.580
a look at this code untar data. image data
loaders from name func. learner. fine tune.

01:16:27.580,01:16:33.740
Untar data. segmentation date loaders. from
label func. learner. fine tune.

01:16:33.740,01:16:39.070
Almost the same code. and this has built a
model that does something. whoa totally different!

01:16:39.070,01:16:42.519
It's something which has taken images.

01:16:42.520,01:16:45.420
This is on the left. this is the labeled data.

01:16:45.420,01:16:51.850
It's got images with color codes to tell you
whether it's a car. or a tree. or a building.

01:16:51.850,01:16:54.201
or a sky. or a line marking or a road.

01:16:54.201,01:16:59.410
And on the right is our model. and our model
has successfully figured out for each pixel.

01:16:59.410,01:17:02.960
is that a car. line marking. a road.

01:17:02.960,01:17:06.520
Now it's only done it in. under 20 seconds
right.

01:17:06.520,01:17:08.480
So it's a very small quick model.

01:17:08.480,01:17:13.059
It's made some mistakes -- like it's missing
this line marking. and some of these cars

01:17:13.060,01:17:15.710
it thinks is house. right?

01:17:15.710,01:17:21.400
But you can see so if you train this for a
few minutes. it's nearly perfect.

01:17:21.400,01:17:26.299
But you can see the basic idea is that we
can very rapidly. with almost exactly the

01:17:26.300,01:17:31.890
same code. create something not that classifies
cats and dogs but does what's called segmentation:

01:17:31.890,01:17:34.950
figures out what every pixel and image is.

01:17:34.950,01:17:36.980
Look. here's the same thing:

01:17:36.980,01:17:39.519
from import star
text loaders from folder

01:17:39.520,01:17:42.530
learner
learn fine-tune

01:17:42.530,01:17:43.830
Same basic code.

01:17:43.830,01:17:49.250
This is now something where we can give it
a sentence and it can figure out whether that

01:17:49.250,01:17:55.380
is expressing a positive or negative sentiment.
and this is actually giving a 93% accuracy

01:17:55.380,01:18:04.440
on that task in about 15 minutes on the IMDb
dataset. which contains thousands of full-length

01:18:04.440,01:18:08.960
movie reviews (in fact. 1000- to 3000-word
reviews).

01:18:08.960,01:18:13.500
This number here that we got with the same
three lines of code would have been the best

01:18:13.500,01:18:19.960
in the world for this task in a very. very.
very popular academics dataset in like 2015

01:18:19.960,01:18:21.670
I think.

01:18:21.670,01:18:31.700
So we are creating world-class models. in
our browser. using the same basic code.

01:18:31.700,01:18:33.389
Here's the same basic steps again:

01:18:33.390,01:18:35.110
from import star
untar data

01:18:35.110,01:18:38.269
tabular data loaders
from csv

01:18:38.270,01:18:39.970
learner fit

01:18:39.970,01:18:52.030
This is now building a model that is predicting
salary based on a csv table containing these

01:18:52.030,01:18:53.030
columns.

01:18:53.030,01:18:56.620
So this is tabular data.

01:18:56.620,01:18:57.620
Here's the same basic steps

01:18:57.620,01:18:59.599
from import *
untar data

01:18:59.600,01:19:02.700
collab data loaders from csv
learner fine-tune

01:19:02.700,01:19:10.849
This is now building something which predicts.
for each combination of a user and a movie.

01:19:10.850,01:19:17.430
what rating do we think that user will give
that movie. based on what other movies they've

01:19:17.430,01:19:18.880
watched and liked in the past.

01:19:18.880,01:19:23.230
This is called collaborative filtering and
is used in recommendation systems.

01:19:23.230,01:19:29.519
So here you've seen some examples of each
of the four applications in fastai.

01:19:29.520,01:19:34.420
And as you'll see throughout this course.
the same basic code and also the same basic

01:19:34.420,01:19:41.700
mathematical and software engineering concepts
allow us to do vastly different things using

01:19:41.700,01:19:43.980
the same basic approach.

01:19:43.980,01:19:47.179
And the reason why is because of Arthur Samuel.

01:19:47.180,01:19:56.840
Because of this basic description of what
it is you can do if only you have a way to

01:19:56.840,01:20:01.630
parameterize a model and you have an update
procedure which can update the weights to

01:20:01.630,01:20:09.850
make you better at your loss function. and
in this case we can use neural networks. which

01:20:09.850,01:20:14.350
are totally flexible functions.

01:20:14.350,01:20:18.250
So that's it for this first lesson.

01:20:18.250,01:20:23.221
It's a little bit shorter than our other lessons
going to be and the reason for that is that

01:20:23.221,01:20:29.849
we are as I mentioned at the start of a global
pandemic here. or at least in the West (in

01:20:29.850,01:20:32.710
other countries they are much further into
it).

01:20:32.710,01:20:36.520
So we spent some time talking about that at
the start of the course and you can find that

01:20:36.520,01:20:39.500
video elsewhere.

01:20:39.500,01:20:45.730
So in the future lessons there will be more
on deep learning.

01:20:45.730,01:20:53.780
So. what I suggest you do over the next week.
before you work on the next lesson. is just

01:20:53.780,01:20:58.469
make sure that you can spin up a GPU server.
you can shut down when it's finished and that

01:20:58.470,01:21:06.010
you can run all of the code here and. as you
go through. see if this is using Python in

01:21:06.010,01:21:14.750
a way you recognise. use the documentation.
use that doc function. do some search on the

01:21:14.750,01:21:20.480
fastai doc. see what it does. see if you can
actually grab the fastai documentation notebooks

01:21:20.480,01:21:22.269
themselves and run them.

01:21:22.270,01:21:26.090
Just try to get comfortable. that you can
if you can know your way around.

01:21:26.090,01:21:30.340
Because the most important thing to do with
this style of learning. this top-down learning.

01:21:30.340,01:21:34.710
is to be able to run experiments and that
means you need to be able to run code.

01:21:34.710,01:21:41.360
So my recommendation is: don't move on until
you can run the code. read the chapter of

01:21:41.360,01:21:47.849
the book. and then go through the questionnaire.

01:21:47.850,01:21:52.980
We still got some more work to do about validation
sets and test sets and transfer learning.

01:21:52.980,01:21:58.360
So you won't be able to do all of it yet but
try to to all the parts you can. based on

01:21:58.360,01:22:01.019
what we've seen of the course so far.

01:22:01.020,01:22:04.130
Rachel. anything you want to add before we
go.

01:22:04.130,01:22:08.350
Okay. so thanks very much for joining us for
lesson one everybody and are really looking

01:22:08.350,01:22:14.880
forward to seeing you next time where we will
learn about transfer learning and then we

01:22:14.880,01:22:22.150
will move on to creating an actual production
version of an application that we can actually

01:22:22.150,01:22:27.500
put out on the Internet and you can start
building apps that you can show your friends

01:22:27.500,01:22:29.960
and they can start playing with.

01:22:29.960,01:22:30.440
Bye Everybody

